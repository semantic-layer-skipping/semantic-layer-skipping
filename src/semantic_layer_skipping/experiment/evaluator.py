import Levenshtein
from data.extractor import AnswerExtractor
from data.loader import DatasetFactory
from experiment.config import EvalConfig
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from rouge_score import rouge_scorer
from structures import EvalStrategy
from tqdm import tqdm


def run_eval_loop(runner, db, thresholds: dict[int, float], config: EvalConfig) -> dict:
    # load prompts
    dataset = DatasetFactory.get_dataset(
        config.dataset, config.split, config.num_samples
    )

    metrics = {
        "exact_matches": 0,
        "fuzzy_matches": 0,
        "task_correctness": 0,
        "baseline_task_correctness": 0,
        "incremental_agreement_rate": 0.0,
        "total_skipped_layers": 0,
        "total_possible_layers": 0,
        "bleu_scores": [],
        "rouge_l_scores": [],
        "samples": [],
    }

    # setup scorers
    rouge_calc = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    chen_cherry = SmoothingFunction()

    for sample in tqdm(dataset, desc="evaluating"):
        # run generation with skipping enabled
        skip_res = runner.generate_with_skipping(
            sample, db, threshold=thresholds, max_new_tokens=config.max_gen_tokens
        )

        sample_data = {
            "id": sample.id,
            "prompt": sample.prompt,
            "generated_text": skip_res.generated_text,
            "skipped_count": skip_res.skipped_layers,
            "generated_token_count": skip_res.generated_token_count,
        }

        def normalise(s):
            if s is None:
                return None
            try:
                # if the answer is a number, convert to float for comparison
                return float(s)
            except ValueError:
                return s.strip().lower()

        # task accuracy check
        if sample.label is not None:
            extracted_answer = AnswerExtractor.extract(
                config.dataset, skip_res.full_text
            )

            is_correct = normalise(extracted_answer) == normalise(sample.label)
            if is_correct:
                metrics["task_correctness"] += 1
            sample_data["label"] = sample.label
            sample_data["extracted_answer"] = extracted_answer
            sample_data["is_correct"] = is_correct

        # strategy: full generation comparison
        # generates the full baseline text to compare quality metrics
        if config.strategy == EvalStrategy.FULL_GENERATION:
            # run baseline without skipping (vector_db=None)
            base_res = runner.generate_with_skipping(
                sample, vector_db=None, max_new_tokens=config.max_gen_tokens
            )

            # baseline task accuracy check
            if sample.label is not None:
                base_extracted = AnswerExtractor.extract(
                    config.dataset, base_res.full_text
                )
                base_is_correct = normalise(base_extracted) == normalise(sample.label)
                if base_is_correct:
                    metrics["baseline_task_correctness"] += 1
                sample_data["baseline_extracted_answer"] = base_extracted
                sample_data["baseline_is_correct"] = base_is_correct

            # calculate metrics
            is_exact = base_res.generated_text == skip_res.generated_text
            similarity = Levenshtein.ratio(
                base_res.generated_text, skip_res.generated_text
            )

            # n-gram metrics (bleu/rouge)
            ref_tokens = base_res.generated_text.split()
            hyp_tokens = skip_res.generated_text.split()
            # handle empty generation edge cases
            if not ref_tokens:
                ref_tokens = [""]
            if not hyp_tokens:
                hyp_tokens = [""]

            bleu = sentence_bleu(
                [ref_tokens], hyp_tokens, smoothing_function=chen_cherry.method1
            )
            rouge = rouge_calc.score(base_res.generated_text, skip_res.generated_text)[
                "rougeL"
            ].fmeasure

            # update counters
            if is_exact:
                metrics["exact_matches"] += 1
            if similarity > 0.9:
                metrics["fuzzy_matches"] += 1

            metrics["bleu_scores"].append(bleu)
            metrics["rouge_l_scores"].append(rouge)

            sample_data.update(
                {
                    "baseline_text": base_res.generated_text,
                    "similarity": similarity,
                    "bleu": bleu,
                    "rouge": rouge,
                }
            )

        # strategy: incremental token match
        # checks if skipped model matches baseline step-by-step
        elif config.strategy == EvalStrategy.INCREMENTAL_MATCH:
            # extract the full sequence of tokens generated by the skipping model
            full_gen_tokens = skip_res.generated_tokens
            base_prompt_text = sample

            matches = 0
            mismatch_indices = []

            # iterate through the sequence generated by the skipping model
            for i, actual_token_id in enumerate(full_gen_tokens):
                # construct the prompt for the baseline model at this step
                if i > 0:
                    current_token_ids = skip_res.prompt_tokens + full_gen_tokens[:i]
                    current_prompt = runner.model.to_string(current_token_ids)
                else:
                    current_prompt = base_prompt_text

                # run baseline (vector_db=None) for 1 generated token
                baseline_res = runner.generate_with_skipping(
                    current_prompt, vector_db=None, max_new_tokens=1
                )

                # extract the single token it predicted
                baseline_token_id = baseline_res.generated_tokens[0]

                # compare
                if baseline_token_id == actual_token_id:
                    matches += 1
                else:
                    mismatch_indices.append(i)

            # calculate agreement
            total_gen = len(full_gen_tokens)
            agreement = matches / total_gen if total_gen > 0 else 1.0

            metrics["incremental_agreement_rate"] += agreement

            sample_data.update(
                {
                    "agreement_rate": agreement,
                    "mismatch_indices": mismatch_indices,
                    "total_tokens_checked": total_gen,
                }
            )

        # calculate efficiency metrics
        # total possible layers = model depth * number of new tokens generated
        n_layers = runner.model.cfg.n_layers
        possible = n_layers * skip_res.generated_token_count

        metrics["total_possible_layers"] += possible
        metrics["total_skipped_layers"] += skip_res.skipped_layers

        metrics["samples"].append(sample_data)

    # aggregation
    total = len(dataset)

    # calculate average agreement if strategy is incremental
    avg_agreement = 0.0
    if config.strategy == EvalStrategy.INCREMENTAL_MATCH and total > 0:
        avg_agreement = metrics["incremental_agreement_rate"] / total

    # calculate average scores for full generation
    avg_bleu = (
        sum(metrics["bleu_scores"]) / total
        if total > 0 and metrics["bleu_scores"]
        else 0
    )
    avg_rouge = (
        sum(metrics["rouge_l_scores"]) / total
        if total > 0 and metrics["rouge_l_scores"]
        else 0
    )

    # efficiency calculations
    total_possible = metrics["total_possible_layers"] + 1e-9
    skip_fraction = metrics["total_skipped_layers"] / total_possible
    theoretical_speedup = 1 / (1 - skip_fraction + 1e-9)
    avg_skipped_per_token = metrics["total_skipped_layers"] / total_possible

    summary = {
        "strategy": str(config.strategy),
        "accuracy": {
            "task_accuracy": metrics["task_correctness"] / total if total > 0 else 0,
            "baseline_task_accuracy": metrics["baseline_task_correctness"] / total
            if total > 0
            else 0,
            "exact_match_pct": metrics["exact_matches"] / total if total > 0 else 0,
            "fuzzy_match_pct": metrics["fuzzy_matches"] / total if total > 0 else 0,
            "avg_bleu": avg_bleu,
            "avg_rouge_l": avg_rouge,
            "incremental_token_accuracy": avg_agreement,
        },
        "efficiency": {
            "avg_skipped_per_token": avg_skipped_per_token,
            "theoretical_speedup": theoretical_speedup,
        },
        "samples": metrics["samples"],
    }

    return summary
