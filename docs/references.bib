
@misc{huang_modes_2025,
	title = {{MoDES}: {Accelerating} {Mixture}-of-{Experts} {Multimodal} {Large} {Language} {Models} via {Dynamic} {Expert} {Skipping}},
	shorttitle = {{MoDES}},
	url = {http://arxiv.org/abs/2511.15690},
	doi = {10.48550/arXiv.2511.15690},
	abstract = {Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88\% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67\% (97.33\% vs. 86.66\%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16\${\textbackslash}times\$ and the decoding time by 1.26\${\textbackslash}times\$.},
	urldate = {2025-12-01},
	publisher = {arXiv},
	author = {Huang, Yushi and Wang, Zining and Yuan, Zhihang and Ding, Yifu and Gong, Ruihao and Guo, Jinyang and Liu, Xianglong and Zhang, Jun},
	month = nov,
	year = {2025},
	note = {arXiv:2511.15690 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/JZWU6T6J/Huang et al. - 2025 - MoDES Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/KERFXAU7/2511.html:text/html},
}

@misc{he_what_2024,
	title = {What {Matters} in {Transformers}? {Not} {All} {Attention} is {Needed}},
	shorttitle = {What {Matters} in {Transformers}?},
	url = {http://arxiv.org/abs/2406.15786},
	doi = {10.48550/arXiv.2406.15786},
	abstract = {While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4{\textbackslash}\% speedup with only a 2.4{\textbackslash}\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90{\textbackslash}\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: {\textbackslash}url\{https://github.com/Shwai-He/LLM-Drop\}.},
	urldate = {2025-12-01},
	publisher = {arXiv},
	author = {He, Shwai and Sun, Guoheng and Shen, Zheyu and Li, Ang},
	month = oct,
	year = {2024},
	note = {arXiv:2406.15786 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/L9VEW9VS/He et al. - 2024 - What Matters in Transformers Not All Attention is Needed.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/E3WDSRWA/2406.html:text/html},
}

@misc{kwon_efficient_2023,
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	doi = {10.48550/arXiv.2309.06180},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06180 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/IQH575JZ/Kwon et al. - 2023 - Efficient Memory Management for Large Language Model Serving with PagedAttention.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/2ZN88XDG/2309.html:text/html},
}

@misc{zheng_sglang_2024,
	title = {{SGLang}: {Efficient} {Execution} of {Structured} {Language} {Model} {Programs}},
	shorttitle = {{SGLang}},
	url = {http://arxiv.org/abs/2312.07104},
	doi = {10.48550/arXiv.2312.07104},
	abstract = {Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E. and Barrett, Clark and Sheng, Ying},
	month = jun,
	year = {2024},
	note = {arXiv:2312.07104 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/DGWWJQJ8/Zheng et al. - 2024 - SGLang Efficient Execution of Structured Language Model Programs.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/BFD4ELAK/2312.html:text/html},
}

@misc{agrawal_taming_2024,
	title = {Taming {Throughput}-{Latency} {Tradeoff} in {LLM} {Inference} with {Sarathi}-{Serve}},
	url = {http://arxiv.org/abs/2403.02310},
	doi = {10.48550/arXiv.2403.02310},
	abstract = {Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency. We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles. Our techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6x gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available at https://github.com/microsoft/sarathi-serve.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S. and Tumanov, Alexey and Ramjee, Ramachandran},
	month = jun,
	year = {2024},
	note = {arXiv:2403.02310 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/K5HPXSXK/Agrawal et al. - 2024 - Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/G6BN5WL6/2403.html:text/html},
}

@inproceedings{yu_orca_2022,
	title = {Orca: {A} {Distributed} {Serving} {System} for {Transformer}-{Based} {Generative} {Models}},
	isbn = {978-1-939133-28-1},
	shorttitle = {Orca},
	url = {https://www.usenix.org/conference/osdi22/presentation/yu},
	language = {en},
	urldate = {2025-12-08},
	author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
	year = {2022},
	pages = {521--538},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/W5E6KM2X/Yu et al. - 2022 - Orca A Distributed Serving System for Transformer-Based Generative Models.pdf:application/pdf},
}

@misc{zhong_distserve_2024,
	title = {{DistServe}: {Disaggregating} {Prefill} and {Decoding} for {Goodput}-optimized {Large} {Language} {Model} {Serving}},
	shorttitle = {{DistServe}},
	url = {http://arxiv.org/abs/2401.09670},
	doi = {10.48550/arXiv.2401.09670},
	abstract = {DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. We find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both. DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU. Our evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4x more requests or 12.6x tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for {\textgreater} 90\% of requests.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
	month = jun,
	year = {2024},
	note = {arXiv:2401.09670 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/2QG8EVBP/Zhong et al. - 2024 - DistServe Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/9CNQ4GSJ/2401.html:text/html},
}

@misc{mei_helix_2025,
	title = {Helix: {Serving} {Large} {Language} {Models} over {Heterogeneous} {GPUs} and {Network} via {Max}-{Flow}},
	shorttitle = {Helix},
	url = {http://arxiv.org/abs/2406.01566},
	doi = {10.48550/arXiv.2406.01566},
	abstract = {This paper introduces Helix, a distributed system for high-throughput, low-latency large language model (LLM) serving in heterogeneous GPU clusters. The key idea behind Helix is to formulate inference computation of LLMs over heterogeneous GPUs and network connections as a max-flow problem on directed, weighted graphs, whose nodes represent GPU instances and edges capture both GPU and network heterogeneity through their capacities. Helix then uses a mixed integer linear programming (MILP) algorithm to discover highly optimized strategies to serve LLMs on heterogeneous GPUs. This approach allows Helix to jointly optimize model placement and request scheduling, two highly entangled tasks in heterogeneous LLM serving. Our evaluation on several heterogeneous clusters ranging from 24 to 42 GPU nodes shows that Helix improves serving throughput by up to 3.3x and reduces prompting and decoding latency by up to 66\% and 24\%, respectively, compared to existing approaches. Helix is available at https://github.com/Thesys-lab/Helix-ASPLOS25.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi},
	month = mar,
	year = {2025},
	note = {arXiv:2406.01566 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/8EE87UUP/Mei et al. - 2025 - Helix Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/969YS4SW/2406.html:text/html},
}

@misc{jiang_thunderserve_2025,
	title = {{ThunderServe}: {High}-performance and {Cost}-efficient {LLM} {Serving} in {Cloud} {Environments}},
	shorttitle = {{ThunderServe}},
	url = {http://arxiv.org/abs/2502.09334},
	doi = {10.48550/arXiv.2502.09334},
	abstract = {Recent developments in large language models (LLMs) have demonstrated their remarkable proficiency in a range of tasks. Compared to in-house homogeneous GPU clusters, deploying LLMs in cloud environments with diverse types of GPUs is crucial for addressing the GPU shortage problem and being more cost-effective. However, the diversity of network environments and various GPU types on the cloud bring difficulties to achieving high-performance serving. In this work, we propose ThunderServe, a high-performance and cost-efficient LLM serving system for heterogeneous cloud environments. We introduce a novel scheduling algorithm, which optimizes the deployment plan of LLM serving to accommodate the heterogeneous resource and network bandwidth conditions in cloud environments. Furthermore, we propose a lightweight re-scheduling mechanism, designed to adapt to fluctuating online conditions (e.g., node failures, workload shifts) without the need for costly restarts of ongoing services. Empirical results in both heterogeneous cloud and homogeneous in-house environments reveal that ThunderServe delivers up to a 2.1\${\textbackslash}times\$ and on average a \$1.7{\textbackslash}times\$ increase in throughput and achieves up to a 2.5\${\textbackslash}times\$ and on average a \$1.5{\textbackslash}times\$ reduction in latency deadlines compared with state-of-the-art systems given the same price budget, suggesting opting for cloud services provides a more cost-efficient solution.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Jiang, Youhe and Fu, Fangcheng and Yao, Xiaozhe and Wang, Taiyi and Cui, Bin and Klimovic, Ana and Yoneki, Eiko},
	month = nov,
	year = {2025},
	note = {arXiv:2502.09334 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/MXTPJX8S/Jiang et al. - 2025 - ThunderServe High-performance and Cost-efficient LLM Serving in Cloud Environments.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/GKB96S9Q/2502.html:text/html},
}

@misc{singh_elasticmoe_2025,
	title = {{ElasticMoE}: {An} {Efficient} {Auto} {Scaling} {Method} for {Mixture}-of-{Experts} {Models}},
	shorttitle = {{ElasticMoE}},
	url = {http://arxiv.org/abs/2510.02613},
	doi = {10.48550/arXiv.2510.02613},
	abstract = {Mixture-of-Experts (MoE) models promise efficient scaling of large language models (LLMs) by activating only a small subset of experts per token, but their parallelized inference pipelines make elastic serving challenging. Existing strategies fall short: horizontal scaling provisions entire replicas of the current configuration, often tens to hundreds of accelerators, leading to coarse granularity, long provisioning delays, and costly overprovisioning. Vertical scaling offers finer adjustments but typically requires instance restarts, incurring downtime. These limitations make current approaches ill-suited for the bursty, short-lived traffic patterns common in cloud deployments. We present ElasticMoE, an elastic scaling framework for MoE LLMs that achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE decouples inference execution from memory operations, enabling scaling steps to proceed concurrently with serving. An HBM Management Module (HMM) reuses weights and KV caches via zero-copy remapping, while high-bandwidth peer-to-peer transfers bring newly added accelerators online without interrupting service. A virtual memory based expert redistribution mechanism migrates MoE experts without costly buffer reallocations, reducing peak memory usage during expert parallelism reconfiguration. Our evaluation on Ascend NPUs with three popular MoE LLMs shows that ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput during scaling, and significantly improves SLO attainment compared to baselines. By enabling fine-grained, concurrent scaling with minimal disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs in dynamic cloud environments.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Singh, Gursimran and Yu, Timothy and Li, Haley and Chen, Cheng and Sadri, Hanieh and Zhang, Qintao and Zhang, Yu and Xiong, Ying and Zhang, Yong and Fan, Zhenan},
	month = oct,
	year = {2025},
	note = {arXiv:2510.02613 [cs]
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/3UAFBSTB/Singh et al. - 2025 - ElasticMoE An Efficient Auto Scaling Method for Mixture-of-Experts Models.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/85NQE8QY/2510.html:text/html},
}

@incollection{angelopoulos_cache_2026,
	title = {Cache {Management} for {Mixture}-of-{Experts} {LLMs} -- extended version},
	volume = {15902},
	url = {http://arxiv.org/abs/2509.02408},
	doi = {10.1007/978-3-031-99872-0_2},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory. In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand. Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.},
	urldate = {2025-12-12},
	author = {Angelopoulos, Spyros and Marchal, Loris and Obrecht, Adrien and Simon, Bertrand},
	year = {2026},
	note = {arXiv:2509.02408 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms},
	pages = {18--32},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/PJ3ZC4V2/Angelopoulos et al. - 2026 - Cache Management for Mixture-of-Experts LLMs -- extended version.pdf:application/pdf},
}

@misc{liu_pikv_2025,
	title = {{PiKV}: {KV} {Cache} {Management} {System} for {Mixture} of {Experts}},
	shorttitle = {{PiKV}},
	url = {http://arxiv.org/abs/2508.06526},
	doi = {10.48550/arXiv.2508.06526},
	abstract = {As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead. We introduce {\textbackslash}textbf\{PiKV\}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages {\textbackslash}textit\{expert-sharded KV storage\} to partition caches across GPUs, {\textbackslash}textit\{PiKV routing\} to reduce token-to-KV access, and a {\textbackslash}textit\{PiKV Scheduling\} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates {\textbackslash}textit\{PiKV Compression\} modules the caching pipeline for acceleration. PiKV is recently publicly available as an open-source software library: {\textbackslash}href\{https://github.com/NoakLiu/PiKV\}\{https://github.com/NoakLiu/PiKV\}. Experiments details is recorded at: {\textbackslash}href\{https://github.com/NoakLiu/PiKV/blob/main/downstream\_tasks/README.md\}\{https://github.com/NoakLiu/PiKV/Experimental{\textbackslash}\_Results\}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see {\textbackslash}href\{https://github.com/NoakLiu/PiKVpress\}\{https://github.com/NoakLiu/PiKVpress\}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Liu, Dong and Yu, Yanxuan and Lengerich, Ben and Wu, Ying Nian and Wang, Xuhong},
	month = aug,
	year = {2025},
	note = {arXiv:2508.06526 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/Z555KI2R/Liu et al. - 2025 - PiKV KV Cache Management System for Mixture of Experts.pdf:application/pdf},
}

@misc{yun_toward_2024,
	title = {Toward {Inference}-optimal {Mixture}-of-{Expert} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.02852},
	doi = {10.48550/arXiv.2404.02852},
	abstract = {Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85\%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Yun, Longfei and Zhuang, Yonghao and Fu, Yao and Xing, Eric P. and Zhang, Hao},
	month = apr,
	year = {2024},
	note = {arXiv:2404.02852 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/IPW4VHV9/Yun et al. - 2024 - Toward Inference-optimal Mixture-of-Expert Large Language Models.pdf:application/pdf},
}

@misc{zhang_duoserve-moe_2025,
	title = {{DuoServe}-{MoE}: {Dual}-{Phase} {Expert} {Prefetch} and {Cache} {Scheduling} for {Efficient} {MoE} {LLM} {Inference}},
	shorttitle = {{DuoServe}-{MoE}},
	url = {http://arxiv.org/abs/2509.07379},
	doi = {10.48550/arXiv.2509.07379},
	abstract = {Large Language Models (LLMs) have demonstrated impressive performance across a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances their capabilities by increasing model width through sparsely activated expert branches, which keeps inference computation efficient. However, the large number of expert weights introduces significant GPU memory pressure, especially in resource-constrained environments such as single-GPU servers. More importantly, MoE inference consists of two fundamentally different stages: a prefill stage where most experts are activated densely, and a decode stage where only a few experts are triggered sparsely. Treating these stages with a uniform scheduling strategy often leads to suboptimal latency and memory usage. To address this, we propose DuoServe-MoE, an inference serving system that explicitly separates prefill and decode stages and applies tailored expert scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a two-stream CUDA pipeline that overlaps expert weight prefetching with the computation of non-MoE layers, limiting expert residency in GPU memory. In the decode stage, a lightweight layer-level predictor trained offline from activation traces is used to prefetch only the most likely activated experts, without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to 7.54 times while keeping peak memory usage at only 15 percent of the full model size.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Zhang, Yuning and Pinkert, Grant and Yang, Nan and Li, Yanli and Yuan, Dong},
	month = sep,
	year = {2025},
	note = {arXiv:2509.07379 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/9VNLKCD3/Zhang et al. - 2025 - DuoServe-MoE Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference.pdf:application/pdf},
}

@misc{huang_mixture_2025,
	title = {Mixture {Compressor} for {Mixture}-of-{Experts} {LLMs} {Gains} {More}},
	url = {http://arxiv.org/abs/2410.06270},
	doi = {10.48550/arXiv.2410.06270},
	abstract = {Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important -- only a small subset is critical. Building on these insights, we propose MC, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization, which formulates the adaptive bit-width allocation as a Linear Programming problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning, which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency. Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC compresses 76.6\% of the model, with only a 3.8\% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15\%, with a performance drop of less than 0.6\%.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Huang, Wei and Liao, Yue and Liu, Jianhui and He, Ruifei and Tan, Haoru and Zhang, Shiming and Li, Hongsheng and Liu, Si and Qi, Xiaojuan},
	month = feb,
	year = {2025},
	note = {arXiv:2410.06270 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/GIKZP8JR/Huang et al. - 2025 - Mixture Compressor for Mixture-of-Experts LLMs Gains More.pdf:application/pdf},
}

@inproceedings{bang_gptcache_2023,
	address = {Singapore},
	title = {{GPTCache}: {An} {Open}-{Source} {Semantic} {Cache} for {LLM} {Applications} {Enabling} {Faster} {Answers} and {Cost} {Savings}},
	shorttitle = {{GPTCache}},
	url = {https://aclanthology.org/2023.nlposs-1.24/},
	doi = {10.18653/v1/2023.nlposs-1.24},
	abstract = {The rise of ChatGPT1 has led to the development of artificial intelligence (AI) applications, particularly those that rely on large language models (LLMs). However, recalling LLM APIs can be expensive, and the response speed may slow down during LLMs' peak times, causing frustration among developers. Potential solutions to this problem include using better LLM models or investing in more computing resources. However, these options may increase product development costs and decrease development speed. GPTCache2 is an open-source semantic cache that stores LLM responses to address this issue. When integrating an AI application with GPTCache, user queries are first sent to GPTCache for a response before being sent to LLMs like ChatGPT. If GPTCache has the answer to a query, it quickly returns the answer to the user without having to query the LLM. This approach saves costs on API recalls and makes response times much faster. For instance, integrating GPTCache with the GPT service offered by OpenAI can increase response speed 2-10 times when the cache is hit. Moreover, network fluctuations will not affect GPTCache's response time, making it highly stable. This paper presents GPTCache and its architecture, how it functions and performs, and the use cases for which it is most advantageous.},
	urldate = {2025-12-12},
	booktitle = {Proceedings of the 3rd {Workshop} for {Natural} {Language} {Processing} {Open} {Source} {Software} ({NLP}-{OSS} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Bang, Fu},
	editor = {Tan, Liling and Milajevs, Dmitrijs and Chauhan, Geeticka and Gwinnup, Jeremy and Rippeth, Elijah},
	month = dec,
	year = {2023},
	pages = {212--218},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/A7UNPWJM/Bang - 2023 - GPTCache An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Saving.pdf:application/pdf},
}

@article{jin_ragcache_2025,
	title = {{RAGCache}: {Efficient} {Knowledge} {Caching} for {Retrieval}-{Augmented} {Generation}},
	volume = {44},
	issn = {0734-2071},
	shorttitle = {{RAGCache}},
	url = {https://dl.acm.org/doi/10.1145/3768628},
	doi = {10.1145/3768628},
	abstract = {Retrieval-Augmented Generation (RAG) has demonstrated substantial advancements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, the retrieval step introduces long sequence generation and extra data dependency, resulting in long end-to-end latency.Our analysis benchmarks current RAG systems and reveals that, while the retrieval step poses performance challenges, it also offers optimization opportunities through its retrieval pattern and streaming search behavior. We propose RAGCache, a latency-optimized serving system tailored for RAG. RAGCache leverages the retrieval pattern to organize and cache the intermediate states of retrieved knowledge in a knowledge tree across the GPU and host memory hierarchy, reducing LLM generation time. RAGCache employs dynamic speculative pipelining to exploit the streaming search behavior, overlapping retrieval with LLM generation to minimize end-to-end latency. We implement RAGCache based on vLLM and Faiss, and evaluate it on both open-source and production datasets. Experimental results demonstrate that RAGCache reduces the time to first token (TTFT) by up to 4× and improves the throughput by up to 2.1× compared to vLLM integrated with Faiss.},
	number = {1},
	urldate = {2025-12-12},
	journal = {ACM Trans. Comput. Syst.},
	author = {Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Shufan and Liu, Xuanzhe and Jin, Xin},
	month = nov,
	year = {2025},
	pages = {2:1--2:27},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/W66ZF4IF/Jin et al. - 2025 - RAGCache Efficient Knowledge Caching for Retrieval-Augmented Generation.pdf:application/pdf},
}

@misc{gim_prompt_2024,
	title = {Prompt {Cache}: {Modular} {Attention} {Reuse} for {Low}-{Latency} {Inference}},
	shorttitle = {Prompt {Cache}},
	url = {http://arxiv.org/abs/2311.04934},
	doi = {10.48550/arXiv.2311.04934},
	abstract = {We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts. Many input prompts have overlapping text segments, such as system messages, prompt templates, and documents provided for context. Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly define such reusable text segments, called prompt modules. The schema ensures positional accuracy during attention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype implementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations. The improvements range from 8x for GPU-based inference to 60x for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
	month = apr,
	year = {2024},
	note = {arXiv:2311.04934 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/EU74D2RZ/Gim et al. - 2024 - Prompt Cache Modular Attention Reuse for Low-Latency Inference.pdf:application/pdf},
}

@misc{douze_faiss_2025,
	title = {The {Faiss} library},
	url = {http://arxiv.org/abs/2401.08281},
	doi = {10.48550/arXiv.2401.08281},
	abstract = {Vector databases typically manage large collections of embedding vectors. Currently, AI applications are growing rapidly, and so is the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper describes the trade-off space of vector search and the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Douze, Matthijs and Guzhva, Alexandr and Deng, Chengqi and Johnson, Jeff and Szilvasy, Gergely and Mazaré, Pierre-Emmanuel and Lomeli, Maria and Hosseini, Lucas and Jégou, Hervé},
	month = oct,
	year = {2025},
	note = {arXiv:2401.08281 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/CLIUE9JP/Douze et al. - 2025 - The Faiss library.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026/},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2025-12-12},
	journal = {Transactions of the Association for Computational Linguistics},
	publisher = {MIT Press},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2019},
	note = {Place: Cambridge, MA},
	pages = {452--466},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/4RKEFAMF/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answering Research.pdf:application/pdf},
}

@inproceedings{he_adaskip_2025,
	series = {{AAAI}'25/{IAAI}'25/{EAAI}'25},
	title = {{AdaSkip}: adaptive sublayer skipping for accelerating long-context {LLM} inference},
	volume = {39},
	isbn = {978-1-57735-897-8},
	shorttitle = {{AdaSkip}},
	url = {https://doi.org/10.1609/aaai.v39i22.34579},
	doi = {10.1609/aaai.v39i22.34579},
	abstract = {Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes AdaSkip, an adaptive sublayer skipping method specifically designed for long-context inference. AdaSkip adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of AdaSkip is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.},
	urldate = {2025-12-12},
	booktitle = {Proceedings of the {Thirty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{Seventh} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence} and {Fifteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {He, Zhuomin and Yao, Yizhen and Zuo, Pengfei and Gao, Bin and Li, Qinya and Zheng, Zhenzhe and Wu, Fan},
	month = feb,
	year = {2025},
	pages = {24050--24058},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/W7IE76MA/He et al. - 2025 - AdaSkip adaptive sublayer skipping for accelerating long-context LLM inference.pdf:application/pdf},
}

@inproceedings{liu_aster_2025,
	address = {New York, NY, USA},
	series = {{MM} '25},
	title = {{ASTER}: {Adaptive} {Dynamic} {Layer}-{Skipping} for {Efficient} {Transformer} {Inference} via {Markov} {Decision} {Process}},
	isbn = {979-8-4007-2035-2},
	shorttitle = {{ASTER}},
	url = {https://dl.acm.org/doi/10.1145/3746027.3755526},
	doi = {10.1145/3746027.3755526},
	abstract = {Transformer-based models have demonstrated remarkable performance in computer vision tasks. However, their increasing model size leads to substantial memory demands and higher latency, hindering practical deployment. This paper presents an adaptive dynamic layer-skipping framework based on Markov Decision Process, which determines optimal computational paths based on the current state of input samples. We introduce a Temporal Importance Difference Reward mechanism to address the credit assignment problem in layer-skipping decisions, and develop a knowledge distillation strategy using learnable cognitive tokens to compensate for information loss. Experiments on various models demonstrate that our method significantly reduces computational costs while maintaining accuracy, offering a practical solution for deploying high-performance Transformer models in resource-constrained environments. The code is available at https://github.com/wjjkhl/ASTER},
	urldate = {2025-12-12},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Fangxin and Wang, Junjie and Yang, Ning and Wang, Zongwu and Zhao, Junping and Jiang, Li and Guan, Haibing},
	month = oct,
	year = {2025},
	pages = {11853--11861},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/3SHWPZ2Z/Liu et al. - 2025 - ASTER Adaptive Dynamic Layer-Skipping for Efficient Transformer Inference via Markov Decision Proce.pdf:application/pdf},
}

@inproceedings{wang_hadskip_2023,
	address = {Singapore},
	title = {{HadSkip}: {Homotopic} and {Adaptive} {Layer} {Skipping} of {Pre}-trained {Language} {Models} for {Efficient} {Inference}},
	shorttitle = {{HadSkip}},
	url = {https://aclanthology.org/2023.findings-emnlp.283/},
	doi = {10.18653/v1/2023.findings-emnlp.283},
	abstract = {Pre-trained language models (LMs) have brought remarkable performance on numerous NLP tasks. However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems. Existing early exiting methods aim to reduce computational complexity by selecting the layer at which to exit, but suffer from the limitation that they have to sequentially traverse through all layers prior to the selected exit layer, which lacks flexibility and degrades their performance. To solve this problem, we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip. HadSkip adaptively selects the layers to skip based on a predefined budget. Specifically, we introduce a learnable gate before each layer of the LM to determine whether the current layer should be skipped. To tackle various challenges in training such as discrete gates and the budget constraint, we propose a fine-grained initialization strategy and homotopic optimization strategy. We conduct extensive experiments on the GLUE benchmark, and experimental results demonstrate the proposed HadSkip outperforms all state-of-the-art baselines significantly.},
	urldate = {2025-12-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Haoyu and Wang, Yaqing and Liu, Tianci and Zhao, Tuo and Gao, Jing},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {4283--4294},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/VGQ5YCLK/Wang et al. - 2023 - HadSkip Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inferenc.pdf:application/pdf},
}

@misc{luo_adaptive_2025,
	title = {Adaptive {Layer}-skipping in {Pre}-trained {LLMs}},
	url = {http://arxiv.org/abs/2503.23798},
	doi = {10.48550/arXiv.2503.23798},
	abstract = {Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, limited attention has been paid to a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive computation in LLMs without modifying their original parameters. Applied to Llama-3-8B, it skips 8 out of 32 layers while maintaining full benchmark performance. Our experiments reveal that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Despite the computational savings, FlexiDepth does not yet achieve wall-clock speedup due to varied skipping patterns and I/O overhead. To inspire future work and advance research on practical speedup, we open-sourced FlexiDepth and a dataset documenting its layer allocation patterns.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Luo, Xuan and Wang, Weizhi and Yan, Xifeng},
	month = oct,
	year = {2025},
	note = {arXiv:2503.23798 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/FXLD5INX/Luo et al. - 2025 - Adaptive Layer-skipping in Pre-trained LLMs.pdf:application/pdf},
}

@misc{noauthor_inside_nodate,
	title = {Inside {vLLM}: {Anatomy} of a {High}-{Throughput} {LLM} {Inference} {System} - {Aleksa} {Gordić}},
	shorttitle = {Inside {vLLM}},
	url = {https://aleksagordic.com/blog/vllm},
	abstract = {From paged attention, continuous batching, prefix caching, specdec, etc. to multi-GPU, multi-node dynamic serving at scale.},
	language = {en},
	urldate = {2025-12-12},
	file = {Snapshot:/Users/yavuz/Zotero/storage/CVL7XQYA/vllm.html:text/html},
}

@misc{yao_cacheblend_2025,
	title = {{CacheBlend}: {Fast} {Large} {Language} {Model} {Serving} for {RAG} with {Cached} {Knowledge} {Fusion}},
	shorttitle = {{CacheBlend}},
	url = {http://arxiv.org/abs/2405.16444},
	doi = {10.48550/arXiv.2405.16444},
	abstract = {Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
	month = apr,
	year = {2025},
	note = {arXiv:2405.16444 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/MVADPHL3/Yao et al. - 2025 - CacheBlend Fast Large Language Model Serving for RAG with Cached Knowledge Fusion.pdf:application/pdf},
}

@misc{jiang_moe-cap_2025,
	title = {{MoE}-{CAP}: {Benchmarking} {Cost}, {Accuracy} and {Performance} of {Sparse} {Mixture}-of-{Experts} {Systems}},
	shorttitle = {{MoE}-{CAP}},
	url = {http://arxiv.org/abs/2412.07067},
	doi = {10.48550/arXiv.2412.07067},
	abstract = {The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Jiang, Yinsicheng and Fu, Yao and Huang, Yeqi and Nie, Ping and Lu, Zhan and Xue, Leyang and He, Congjie and Sit, Man-Kit and Xue, Jilong and Dong, Li and Miao, Ziming and Du, Dayou and Xu, Tairan and Zou, Kai and Ponti, Edoardo and Mai, Luo},
	month = nov,
	year = {2025},
	note = {arXiv:2412.07067 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/G4BJJ4F4/Jiang et al. - 2025 - MoE-CAP Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems.pdf:application/pdf},
}

@misc{thyssens_routing_2023,
	title = {Routing {Arena}: {A} {Benchmark} {Suite} for {Neural} {Routing} {Solvers}},
	shorttitle = {Routing {Arena}},
	url = {http://arxiv.org/abs/2310.04140},
	doi = {10.48550/arXiv.2310.04140},
	abstract = {Neural Combinatorial Optimization has been researched actively in the last eight years. Even though many of the proposed Machine Learning based approaches are compared on the same datasets, the evaluation protocol exhibits essential flaws and the selection of baselines often neglects State-of-the-Art Operations Research approaches. To improve on both of these shortcomings, we propose the Routing Arena, a benchmark suite for Routing Problems that provides a seamless integration of consistent evaluation and the provision of baselines and benchmarks prevalent in the Machine Learning- and Operations Research field. The proposed evaluation protocol considers the two most important evaluation cases for different applications: First, the solution quality for an a priori fixed time budget and secondly the anytime performance of the respective methods. By setting the solution trajectory in perspective to a Best Known Solution and a Base Solver's solutions trajectory, we furthermore propose the Weighted Relative Average Performance (WRAP), a novel evaluation metric that quantifies the often claimed runtime efficiency of Neural Routing Solvers. A comprehensive first experimental evaluation demonstrates that the most recent Operations Research solvers generate state-of-the-art results in terms of solution quality and runtime efficiency when it comes to the vehicle routing problem. Nevertheless, some findings highlight the advantages of neural approaches and motivate a shift in how neural solvers should be conceptualized.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Thyssens, Daniela and Dernedde, Tim and Falkner, Jonas K. and Schmidt-Thieme, Lars},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04140 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/EJYM7L3C/Thyssens et al. - 2023 - Routing Arena A Benchmark Suite for Neural Routing Solvers.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/XVZ74JLC/2310.html:text/html},
}

@misc{noauthor_vllmbenchmarks_nodate,
	title = {vllm/benchmarks at main · vllm-project/vllm},
	url = {https://github.com/vllm-project/vllm/tree/main/benchmarks},
	abstract = {A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm},
	language = {en},
	urldate = {2025-12-12},
	journal = {GitHub},
	file = {Snapshot:/Users/yavuz/Zotero/storage/MX8C44RN/benchmarks.html:text/html},
}

@misc{noauthor_anon8231489123sharegpt_vicuna_unfiltered_2024,
	title = {anon8231489123/{ShareGPT}\_Vicuna\_unfiltered · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-12-12},
	month = may,
	year = {2024},
	file = {Snapshot:/Users/yavuz/Zotero/storage/BLCMAURQ/ShareGPT_Vicuna_unfiltered.html:text/html},
}

@misc{noauthor_lmsyslmsys-chat-1m_2025,
	title = {lmsys/lmsys-chat-1m · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/lmsys/lmsys-chat-1m},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-12-12},
	month = apr,
	year = {2025},
}

@misc{noauthor_semantic-routerbench_nodate,
	title = {semantic-router/bench at main · vllm-project/semantic-router},
	url = {https://github.com/vllm-project/semantic-router/tree/main/bench},
	abstract = {Intelligent Router for Mixture-of-Models. Contribute to vllm-project/semantic-router development by creating an account on GitHub.},
	language = {en},
	urldate = {2025-12-12},
	journal = {GitHub},
}

@misc{noauthor_gptcacheexamplesbenchmarkbenchmark_sqlite_faiss_onnxpy_nodate,
	title = {{GPTCache}/examples/benchmark/benchmark\_sqlite\_faiss\_onnx.py at main · zilliztech/{GPTCache}},
	url = {https://github.com/zilliztech/GPTCache/blob/main/examples/benchmark/benchmark_sqlite_faiss_onnx.py},
	abstract = {Semantic cache for LLMs. Fully integrated with LangChain and llama\_index.  - zilliztech/GPTCache},
	language = {en},
	urldate = {2025-12-12},
	journal = {GitHub},
}

@misc{liu_dynamic_2025,
	title = {Dynamic {Rebatching} for {Efficient} {Early}-{Exit} {Inference} with {DREX}},
	url = {http://arxiv.org/abs/2512.15705},
	doi = {10.48550/arXiv.2512.15705},
	abstract = {Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12\% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.},
	urldate = {2025-12-20},
	publisher = {arXiv},
	author = {Liu, Xuting and Alexander, Daniel and Kakarla, Siva Kesava Reddy and Arzani, Behnaz and Liu, Vincent},
	month = dec,
	year = {2025},
	note = {arXiv:2512.15705 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/SJHQQMI4/Liu et al. - 2025 - Dynamic Rebatching for Efficient Early-Exit Inference with DREX.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/IUE88TNU/2512.html:text/html},
}

@misc{liu_retrievalattention_2024,
	title = {{RetrievalAttention}: {Accelerating} {Long}-{Context} {LLM} {Inference} via {Vector} {Retrieval}},
	shorttitle = {{RetrievalAttention}},
	url = {http://arxiv.org/abs/2409.10516},
	doi = {10.48550/arXiv.2409.10516},
	abstract = {Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3\% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.},
	urldate = {2025-12-20},
	publisher = {arXiv},
	author = {Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
	month = dec,
	year = {2024},
	note = {arXiv:2409.10516 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/MGTUFZR8/Liu et al. - 2024 - RetrievalAttention Accelerating Long-Context LLM Inference via Vector Retrieval.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/WGZ4CZXM/2409.html:text/html},
}

@misc{yao_cacheblend_2025-1,
	title = {{CacheBlend}: {Fast} {Large} {Language} {Model} {Serving} for {RAG} with {Cached} {Knowledge} {Fusion}},
	shorttitle = {{CacheBlend}},
	url = {http://arxiv.org/abs/2405.16444},
	doi = {10.48550/arXiv.2405.16444},
	abstract = {Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.},
	urldate = {2025-12-20},
	publisher = {arXiv},
	author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
	month = apr,
	year = {2025},
	note = {arXiv:2405.16444 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/GNYZ8HC2/Yao et al. - 2025 - CacheBlend Fast Large Language Model Serving for RAG with Cached Knowledge Fusion.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/MDYB7PXQ/2405.html:text/html},
}

@misc{bae_fast_2023,
	title = {Fast and {Robust} {Early}-{Exiting} {Framework} for {Autoregressive} {Language} {Models} with {Synchronized} {Parallel} {Decoding}},
	url = {http://arxiv.org/abs/2310.05424},
	doi = {10.48550/arXiv.2310.05424},
	abstract = {To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.},
	urldate = {2025-12-22},
	publisher = {arXiv},
	author = {Bae, Sangmin and Ko, Jongwoo and Song, Hwanjun and Yun, Se-Young},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05424 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/A69RL389/Bae et al. - 2023 - Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Paralle.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/N3979FRF/2310.html:text/html},
}

@misc{liu_lmcache_2025,
	title = {{LMCache}: {An} {Efficient} {KV} {Cache} {Layer} for {Enterprise}-{Scale} {LLM} {Inference}},
	shorttitle = {{LMCache}},
	url = {http://arxiv.org/abs/2510.09665},
	doi = {10.48550/arXiv.2510.09665},
	abstract = {KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.},
	language = {en},
	urldate = {2025-12-22},
	publisher = {arXiv},
	author = {Liu, Yuhan and Cheng, Yihua and Yao, Jiayi and An, Yuwei and Chen, Xiaokun and Feng, Shaoting and Huang, Yuyang and Shen, Samuel and Zhang, Rui and Du, Kuntai and Jiang, Junchen},
	month = dec,
	year = {2025},
	note = {arXiv:2510.09665 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/Users/yavuz/Zotero/storage/8VF66UK5/Liu et al. - 2025 - LMCache An Efficient KV Cache Layer for Enterprise-Scale LLM Inference.pdf:application/pdf},
}

@misc{huang_mixture_2025-1,
	title = {Mixture {Compressor} for {Mixture}-of-{Experts} {LLMs} {Gains} {More}},
	url = {http://arxiv.org/abs/2410.06270},
	doi = {10.48550/arXiv.2410.06270},
	abstract = {Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important -- only a small subset is critical. Building on these insights, we propose MC, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization, which formulates the adaptive bit-width allocation as a Linear Programming problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning, which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency. Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC compresses 76.6\% of the model, with only a 3.8\% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15\%, with a performance drop of less than 0.6\%.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Huang, Wei and Liao, Yue and Liu, Jianhui and He, Ruifei and Tan, Haoru and Zhang, Shiming and Li, Hongsheng and Liu, Si and Qi, Xiaojuan},
	month = feb,
	year = {2025},
	note = {arXiv:2410.06270 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/ZIKHCT2U/Huang et al. - 2025 - Mixture Compressor for Mixture-of-Experts LLMs Gains More.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/3EAR3N9J/2410.html:text/html},
}

@misc{men_shortgpt_2024,
	title = {{ShortGPT}: {Layers} in {Large} {Language} {Models} are {More} {Redundant} {Than} {You} {Expect}},
	shorttitle = {{ShortGPT}},
	url = {http://arxiv.org/abs/2403.03853},
	doi = {10.48550/arXiv.2403.03853},
	abstract = {As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.},
	urldate = {2026-01-07},
	publisher = {arXiv},
	author = {Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
	month = oct,
	year = {2024},
	note = {arXiv:2403.03853 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/JRN6HPNF/Men et al. - 2024 - ShortGPT Layers in Large Language Models are More Redundant Than You Expect.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/4RH6IYH5/2403.html:text/html},
}

@misc{khandelwal_generalization_2020,
	title = {Generalization through {Memorization}: {Nearest} {Neighbor} {Language} {Models}},
	shorttitle = {Generalization through {Memorization}},
	url = {http://arxiv.org/abs/1911.00172},
	doi = {10.48550/arXiv.1911.00172},
	abstract = {We introduce \$k\$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a \$k\$-nearest neighbors (\$k\$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our \$k\$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
	urldate = {2026-01-12},
	publisher = {arXiv},
	author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
	month = feb,
	year = {2020},
	note = {arXiv:1911.00172 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/U24KSJLR/Khandelwal et al. - 2020 - Generalization through Memorization Nearest Neighbor Language Models.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/8GNAUC7S/1911.html:text/html},
}

@misc{song_knn-ssd_2025,
	title = {{KNN}-{SSD}: {Enabling} {Dynamic} {Self}-{Speculative} {Decoding} via {Nearest} {Neighbor} {Layer} {Set} {Optimization}},
	shorttitle = {{KNN}-{SSD}},
	url = {http://arxiv.org/abs/2505.16162},
	doi = {10.48550/arXiv.2505.16162},
	abstract = {Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate the inference of large language models (LLMs) without compromising generation quality. It works by efficiently drafting multiple tokens using a compact model and then verifying them in parallel using the target LLM. Notably, Self-Speculative Decoding proposes skipping certain layers to construct the draft model, which eliminates the need for additional parameters or training. Despite its strengths, we observe in this work that drafting with layer skipping exhibits significant sensitivity to domain shifts, leading to a substantial drop in acceleration performance. To enhance the domain generalizability of this paradigm, we introduce KNN-SSD, an algorithm that leverages K-Nearest Neighbor (KNN) search to match different skipped layers with various domain inputs. We evaluated our algorithm in various models and multiple tasks, observing that its application leads to 1.3x-1.6x speedup in LLM inference.},
	urldate = {2026-01-12},
	publisher = {arXiv},
	author = {Song, Mingbo and Xia, Heming and Zhang, Jun and Leong, Chak Tou and Xu, Qiancheng and Li, Wenjie and Li, Sujian},
	month = may,
	year = {2025},
	note = {arXiv:2505.16162 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/SKW23UUN/Song et al. - 2025 - KNN-SSD Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/9VVCN5HC/2505.html:text/html},
}

@misc{men_shortgpt_2024-1,
	title = {{ShortGPT}: {Layers} in {Large} {Language} {Models} are {More} {Redundant} {Than} {You} {Expect}},
	shorttitle = {{ShortGPT}},
	url = {http://arxiv.org/abs/2403.03853},
	doi = {10.48550/arXiv.2403.03853},
	abstract = {As Large Language Models (LLMs) continue to advance in performance, their size has increased significantly, with current LLMs containing billions or even trillions of parameters. In this study, we identify notable redundancy across the layers of LLMs, where some layers contribute minimally to overall network functionality. To quantify this, we introduce a metric called Block Influence (BI) which use the similarity between layer’s input and output to measure the importance of each layer. Based on the observation of layer redundancy, we propose a straightforward pruning method: layer removal, which eliminates redundant layers based on their BI scores. Our approach, termed ShortGPT, demonstrates superior performance over previous state-of-the-art pruning methods. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy across layers, not only in transformer models but also in non-transformer models. We hope this work will contribute to future research in LLM compression.},
	language = {en},
	urldate = {2026-01-13},
	publisher = {arXiv},
	author = {Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
	month = oct,
	year = {2024},
	note = {arXiv:2403.03853 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/yavuz/Zotero/storage/E8XEMIEV/Men et al. - 2024 - ShortGPT Layers in Large Language Models are More Redundant Than You Expect.pdf:application/pdf},
}

@misc{rajabzadeh_lora-drop_2026,
	title = {{LoRA}-{Drop}: {Temporal} {LoRA} {Decoding} for {Efficient} {LLM} {Inference}},
	shorttitle = {{LoRA}-{Drop}},
	url = {http://arxiv.org/abs/2601.02569},
	doi = {10.48550/arXiv.2601.02569},
	abstract = {Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present {\textbackslash}textbf\{LoRA-Drop\}, a plug-and-play inference framework that accelerates decoding by applying a {\textbackslash}emph\{temporal compute schedule\} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic {\textbackslash}emph\{refresh\} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across {\textbackslash}textbf\{LLaMA2-7B\}, {\textbackslash}textbf\{LLaMA3-8B\}, {\textbackslash}textbf\{Qwen2.5-7B\}, and {\textbackslash}textbf\{Qwen2.5-14B\}, LoRA-Drop achieves up to {\textbackslash}textbf\{2.6\${\textbackslash}times\$ faster decoding\} and {\textbackslash}textbf\{45--55{\textbackslash}\% KV-cache reduction\} while staying within {\textbackslash}textbf\{0.5 percentage points (pp)\} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent {\textbackslash}emph\{safe zone\} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.},
	urldate = {2026-01-16},
	publisher = {arXiv},
	author = {Rajabzadeh, Hossein and Dialameh, Maryam and Park, Chul B. and Kim, Il-Min and Kwon, Hyock Ju},
	month = jan,
	year = {2026},
	note = {arXiv:2601.02569 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/ABE4ZNDK/Rajabzadeh et al. - 2026 - LoRA-Drop Temporal LoRA Decoding for Efficient LLM Inference.pdf:application/pdf},
}

@misc{fu_lazyllm_2024,
	title = {{LazyLLM}: {Dynamic} {Token} {Pruning} for {Efficient} {Long} {Context} {LLM} {Inference}},
	shorttitle = {{LazyLLM}},
	url = {http://arxiv.org/abs/2407.14057},
	doi = {10.48550/arXiv.2407.14057},
	abstract = {The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.},
	urldate = {2026-01-27},
	publisher = {arXiv},
	author = {Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
	month = jul,
	year = {2024},
	note = {arXiv:2407.14057 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/HMZDSGWE/Fu et al. - 2024 - LazyLLM Dynamic Token Pruning for Efficient Long Context LLM Inference.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/SLJAX4E4/2407.html:text/html},
}

@inproceedings{zhang_investigating_2024,
	address = {Miami, Florida, US},
	title = {Investigating {Layer} {Importance} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.blackboxnlp-1.29/},
	doi = {10.18653/v1/2024.blackboxnlp-1.29},
	abstract = {Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in safety-critical scenarios and hindered the development of better models. In this study, we advance the understanding of LLM by investigating the significance of individual layers in LLMs. We propose an efficient sampling method to faithfully evaluate the importance of layers using Shapley values, a widely used explanation framework in feature attribution and data valuation. In addition, we conduct layer ablation experiments to assess the performance degradation resulting from the exclusion of specific layers. Our findings reveal the existence of cornerstone layers, wherein certain early layers can exhibit a dominant contribution over others. Removing one cornerstone layer leads to a drastic collapse of the model performance, often reducing it to random guessing. Conversely, removing non-cornerstone layers results in only marginal performance changes. This study identifies cornerstone layers in LLMs and underscores their critical role for future research.},
	urldate = {2026-01-27},
	booktitle = {Proceedings of the 7th {BlackboxNLP} {Workshop}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yang and Dong, Yanfei and Kawaguchi, Kenji},
	editor = {Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie},
	month = nov,
	year = {2024},
	pages = {469--479},
	file = {Full Text PDF:/Users/yavuz/Zotero/storage/E8MJF6ED/Zhang et al. - 2024 - Investigating Layer Importance in Large Language Models.pdf:application/pdf},
}

@misc{zheng_alpa_2022,
	title = {Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for {Distributed} {Deep} {Learning}},
	shorttitle = {Alpa},
	url = {http://arxiv.org/abs/2201.12023},
	doi = {10.48550/arXiv.2201.12023},
	abstract = {Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism. Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations. They do not suffice to scale out complex DL models on distributed compute devices. Alpa distributes the training of large DL models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms. Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans. Alpa designs a number of compilation passes to automatically derive efficient parallel execution plans at each parallelism level. Alpa implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices. Our evaluation shows Alpa generates parallelization plans that match or outperform hand-tuned model-parallel training systems even on models they are designed for. Unlike specialized systems, Alpa also generalizes to models with heterogeneous architectures and models without manually-designed plans. Alpa's source code is publicly available at https://github.com/alpa-projects/alpa},
	urldate = {2026-01-27},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion},
	month = jun,
	year = {2022},
	note = {arXiv:2201.12023 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/LGM8ANSY/Zheng et al. - 2022 - Alpa Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/JDCMYVAS/2201.html:text/html},
}

@misc{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: {Training} {Multi}-{Billion} {Parameter} {Language} {Models} {Using} {Model} {Parallelism}},
	shorttitle = {Megatron-{LM}},
	url = {http://arxiv.org/abs/1909.08053},
	doi = {10.48550/arXiv.1909.08053},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
	urldate = {2026-01-27},
	publisher = {arXiv},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08053 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/4K7L3GI6/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf:application/pdf;Snapshot:/Users/yavuz/Zotero/storage/UNEKUSV8/1909.html:text/html},
}

@misc{narayanan_efficient_2021,
	title = {Efficient {Large}-{Scale} {Language} {Model} {Training} on {GPU} {Clusters} {Using} {Megatron}-{LM}},
	url = {http://arxiv.org/abs/2104.04473},
	doi = {10.48550/arXiv.2104.04473},
	abstract = {Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52\% of theoretical peak).},
	language = {en},
	urldate = {2026-01-27},
	publisher = {arXiv},
	author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
	month = aug,
	year = {2021},
	note = {arXiv:2104.04473 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {PDF:/Users/yavuz/Zotero/storage/GNGNSA8B/Narayanan et al. - 2021 - Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.pdf:application/pdf},
}

@misc{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	shorttitle = {{GPipe}},
	url = {http://arxiv.org/abs/1811.06965},
	doi = {10.48550/arXiv.1811.06965},
	abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
	urldate = {2026-01-27},
	publisher = {arXiv},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	month = jul,
	year = {2019},
	note = {arXiv:1811.06965 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/yavuz/Zotero/storage/DHB4RD5R/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks using Pipeline Parallelism.pdf:application/pdf},
}
